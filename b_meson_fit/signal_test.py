import numpy.testing as nt
import tensorflow.compat.v2 as tf
# Import this separately as its old Tensorflow v1 code
from tensorflow.contrib import integrate as tf_integrate
import unittest

import b_meson_fit.coeffs as bmfc
import b_meson_fit.signal as bmfs

tf.enable_v2_behavior()


class TestSignal(unittest.TestCase):

    # Fields are: name, list of coefficients, "true" decay rate (As generated by odeint_fixed() attempt across all vars)
    # To add more to this list:
    #  1. Add a new line with "true" decay rate set to something like 1.0.
    #  2. Run the test_data/signal_integrator.py file to get the "true" values
    #  3. Set the found decay rate in your new line
    test_coeffs = [
        ('signal', bmfc.signal(), 487.20193,),
        ('ones', [tf.constant(1.0)] * bmfc.count, 2687.5159,),
        ('integers', [tf.constant(float(i)) for i in range(int(-bmfc.count / 2), int(bmfc.count / 2))], 436425.156,),
        ('minus_point_ones', [tf.constant(-0.1)] * bmfc.count, 26.875162,),
    ]

    def test_decay_rate_integration_methods_approx_equal(self):
        """
        Check that the _integrate_decay_rate() method that integrates a previously angle-integrated decay rate
        over q^2 returns something approximately equal to running odeint_fixed() over all variables.

        Checks to within 1% as both methods use bins and add errors.
        """
        # Check for different lists of coefficients
        for c_name, coeffs, expected_decay_rate in self.test_coeffs:
            with self.subTest(c_name=c_name):
                actual = bmfs._integrate_decay_rate(coeffs)
                # Check values are the same to within 0.1%
                nt.assert_allclose(expected_decay_rate, actual.numpy(), atol=0, rtol=0.01)

    def test_integrate_decay_rate_within_tolerance(self):
        """
        Check that the tolerances set in _integrate_decay_rate() have not been relaxed so much that
        they mess up the accuracy more than 0.1% from using odeint() on the previously angle integrated decay rate.
        """
        for c_name, coeffs, _ in self.test_coeffs:
            with self.subTest(c_name=c_name):
                true = tf_integrate.odeint(
                    lambda _, q2: bmfs.decay_rate_angle_integrated(coeffs, q2),
                    0.0,
                    tf.stack([bmfs.q2_min, bmfs.q2_max]),
                )[1]

                ours = bmfs._integrate_decay_rate(coeffs)

                nt.assert_allclose(true.numpy(), ours.numpy(), atol=0, rtol=0.001)

    def test_generate_returns_correct_shape(self):
        """Check generate() returns a tensor of shape (events_total, 4)"""
        events = bmfs.generate(bmfc.signal(), 123_456)
        self.longMessage = True
        self.assertEqual(123_456, tf.shape(events)[0].numpy())
        self.assertEqual(4, tf.shape(events)[1].numpy())

    def test_frac_s_methods_approx_equal(self):
        """
        Check decay_rate_frac_s() returns approximately the same values as a method that uses the moduli of the
        amplitudes. It is almost equal for the signal coefficients, but drifts more for other coeffs. I think this is
        because:
         * Coeffs are not arbitrary but actually depend on each other so artificial ones (e.g. integers)
            are not realistic
         *  a_t_l and a_t_r have been neglected
        """
        q2 = tf.linspace(bmfs.q2_min, bmfs.q2_max, 9)

        for c_name, coeffs, _ in self.test_coeffs:
            with self.subTest(c_name=c_name):
                decay_rate_frac_s = bmfs.decay_rate_frac_s(coeffs, q2)
                modulus_frac_s = bmfs.modulus_frac_s(coeffs, q2)
                for i in range(tf.shape(q2)[0].numpy()):
                    nt.assert_allclose(
                        decay_rate_frac_s[i].numpy(),
                        modulus_frac_s[i].numpy(),
                        atol=0,
                        rtol=0.025,
                        err_msg='{} not within 2.5% for q2 {}'.format(c_name, q2[i].numpy()),
                    )


if __name__ == '__main__':
    unittest.main()
